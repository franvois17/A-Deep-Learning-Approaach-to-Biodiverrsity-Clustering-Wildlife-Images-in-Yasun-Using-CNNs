{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Metadata Extraction <b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hashlib\n",
    "import exifread\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the list of standard metadata fields\n",
    "STANDARD_METADATA_FIELDS = [\n",
    "    'Image Width', 'Image Length', 'Image Make', 'Image Model', 'Orientation',\n",
    "    'Image DateTime', 'Image XResolution', 'Image YResolution', 'Image ResolutionUnit', 'Software',\n",
    "    'Artist', 'Image YCbCrPositioning', 'EXIF ExposureTime', 'FNumber', 'ExposureProgram',\n",
    "    'ISO', 'EXIF ExifVersion', 'EXIF DateTimeOriginal', 'EXIF DateTimeDigitized',\n",
    "    'ShutterSpeedValue', 'ApertureValue', 'BrightnessValue', 'ExposureBiasValue',\n",
    "    'MaxApertureValue', 'MeteringMode', 'LightSource', 'EXIF Flash', 'FocalLength',\n",
    "    'EXIF FlashPixVersion', 'EXIF ColorSpace', 'PixelXDimension', 'PixelYDimension', 'RelatedSoundFile',\n",
    "    'FocalPlaneXResolution', 'FocalPlaneYResolution', 'FocalPlaneResolutionUnit',\n",
    "    'SensingMethod', 'FileSource', 'SceneType', 'DigitalZoomRatio', 'FocalLengthIn35mmFilm', 'GainControl',\n",
    "    'Contrast', 'Saturation', 'Sharpness', 'DeviceSettingDescription', 'SubjectDistanceRange', 'ImageUniqueID',\n",
    "    'GPSInfo', 'Image ExifOffset', 'EXIF ISOSpeedRatings', 'EXIF ComponentsConfiguration', 'EXIF ExifImageWidth',\n",
    "    'EXIF ExifImageLength', 'EXIF ExposureMode', 'EXIF WhiteBalance', 'EXIF SceneCaptureType'\n",
    "]\n",
    "\n",
    "# Function to calculate the SHA-256 hash of a file\n",
    "def sha256sum(filename):\n",
    "    h = hashlib.sha256()\n",
    "    with open(filename, 'rb') as file:\n",
    "        while True:\n",
    "            chunk = file.read(h.block_size)\n",
    "            if not chunk:\n",
    "                break\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "# Function to get image metadata\n",
    "def get_image_metadata(filename):\n",
    "    with open(filename, 'rb') as file:\n",
    "        exif_tags = exifread.process_file(file, details=False)\n",
    "    \n",
    "    # Convert tags to a serializable format\n",
    "    metadata = {}\n",
    "    for tag, value in exif_tags.items():\n",
    "        # Convert the tag value to a string if it's not serializable\n",
    "        try:\n",
    "            json.dumps(value)\n",
    "            metadata[tag] = value\n",
    "        except TypeError:\n",
    "            metadata[tag] = str(value)\n",
    "\n",
    "    return metadata\n",
    "\n",
    "# Function to check if a file is hidden\n",
    "def is_hidden(filepath):\n",
    "    return filepath.startswith('.')\n",
    "\n",
    "# Function to check if a file is an image\n",
    "def is_image_file(filename):\n",
    "    image_extensions = {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.webp'}\n",
    "    return os.path.splitext(filename)[1].lower() in image_extensions\n",
    "\n",
    "# Main function to generate the report and save data to a JSON file\n",
    "def generate_file_report(root_directory, output_file):\n",
    "    data = []\n",
    "    file_id = 1\n",
    "    prefix_to_remove = '/' # dataset path\n",
    "\n",
    "    start_time = time.time()  # Start time\n",
    "\n",
    "    for root, dirs, files in os.walk(root_directory):\n",
    "        for file in tqdm(files, desc=\"Processing images\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            if is_hidden(file) or not is_image_file(file):\n",
    "                continue\n",
    "            file_hash = sha256sum(file_path)\n",
    "            metadata = get_image_metadata(file_path)\n",
    "            # Remove the prefix from the path\n",
    "            file_path_rel = file_path.replace(prefix_to_remove, '', 1)\n",
    "            row = {'ID': file_id, 'SHA256': file_hash, 'Path': file_path_rel, 'Metadata': metadata}\n",
    "            data.append(row)\n",
    "            file_id += 1\n",
    "\n",
    "    end_time = time.time()  # End time\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    with open(output_file, 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4)\n",
    "\n",
    "    print(f\"Process completed in {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# path where the dataset is located and where the .json will be stored\n",
    "if __name__ == '__main__':\n",
    "    generate_file_report('/', '/') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Detector Merge with Metadata<b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merges two JSON files based on image IDs and paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the JSON files\n",
    "ordered_json_path = '/home/fbaquero/tesis/ordered_json_final.json'\n",
    "dragon_metadata_path = '/home/fbaquero/tesis/dragonMetaData.json'\n",
    "output_path = '/home/fbaquero/tesis/mergedDragon.json'\n",
    "\n",
    "# Function to read a JSON file\n",
    "def read_json(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "# Function to save a JSON file\n",
    "def write_json(data, file_path):\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "# Read the JSON files\n",
    "ordered_data = read_json(ordered_json_path)\n",
    "dragon_data = read_json(dragon_metadata_path)\n",
    "\n",
    "# Create a dictionary for quick access to dragonData based on Path\n",
    "dragon_dict = {}\n",
    "for item in dragon_data:\n",
    "    full_path = os.path.join('/media/databases/tiputini', item['Path'])\n",
    "    dragon_dict[full_path] = item\n",
    "\n",
    "# Perform the merge based on img_id and Path\n",
    "merged_data = []\n",
    "for item in ordered_data['annotations']:\n",
    "    img_id = item['img_id']\n",
    "    if img_id in dragon_dict:\n",
    "        merged_item = {**item, **dragon_dict[img_id]}\n",
    "        merged_data.append(merged_item)\n",
    "    else:\n",
    "        merged_data.append(item)\n",
    "\n",
    "# Save the merged file\n",
    "write_json(merged_data, output_path)\n",
    "\n",
    "print(f\"The merged file has been saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code merges metadata and detection information from a JSON file into a new structured format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from the JSON file\n",
    "with open('mergedDragon.json', 'r') as file: # Open the json resulting from the megadetector merge with metadata \n",
    "    data = json.load(file)\n",
    "\n",
    "merged_data = []\n",
    "\n",
    "# Iterate over each object in the data list\n",
    "for item in data:\n",
    "    # Create the merged item using the provided format\n",
    "    merged_item = {\n",
    "        \"metadata\": {\n",
    "            \"image\": item['Metadata'],\n",
    "            \"other\": {\n",
    "                \"location\": {},\n",
    "                \"label_location\": {}\n",
    "            }\n",
    "        },\n",
    "        \"id_sha_256\": item['SHA256'],\n",
    "        \"paths\": [item['Path']],\n",
    "        \"detectors\": {\n",
    "            \"megadetectorV5\": {\n",
    "                \"researcher\": \"Oscar Cajamarca, Jenner Baquero\",\n",
    "                \"output\": {\n",
    "                    \"file\": item['img_id'],\n",
    "                    \"detections\": [{\n",
    "                        \"category\": item['category'],\n",
    "                        \"confidence\": item['confidence'],\n",
    "                        \"bbox\": item['bbox'],\n",
    "                        \"detection_categories\": {\n",
    "                            \"0\": \"animal\",\n",
    "                            \"1\": \"person\",\n",
    "                            \"2\": \"vehicle\"\n",
    "                        },\n",
    "                        \"info\": {\n",
    "                            \"detection_completion_time\": \"2024-05-08 20:00:00\",\n",
    "                            \"format_version\": \"1.3\",\n",
    "                            \"detector\": \"md_v5a.0.0.pt\",\n",
    "                            \"detector_metadata\": {\n",
    "                                \"megadetector_version\": \"v5a.0.0\",\n",
    "                                \"typical_detection_threshold\": 0.2,\n",
    "                                \"conservative_detection_threshold\": 0.05\n",
    "                            }\n",
    "                        }\n",
    "                    }]\n",
    "                }\n",
    "            },\n",
    "            \"another_detector\": {\n",
    "                \"researcher\": {},\n",
    "                \"output\": {}\n",
    "            },\n",
    "            \"ground_truth\": {\n",
    "                \"researcher\": {},\n",
    "                \"output\": {}\n",
    "            },\n",
    "        },\n",
    "        \"segmentation_masks\": {}\n",
    "    }\n",
    "    \n",
    "    # Add the merged item to the list of merged data\n",
    "    merged_data.append(merged_item)\n",
    "\n",
    "# Save the merged data to a new JSON file\n",
    "with open('Dragon_merge_v1.json', 'w') as file: # save the resulting json \n",
    "    json.dump(merged_data, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merges paths in a JSON file based on SHA-256 identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Path to the original JSON file\n",
    "input_file_path = '/home/Dragon_merge_v1.json'\n",
    "# Path to the resulting JSON file\n",
    "output_file_path = '/home/Dragonpathsmerged.json'\n",
    "\n",
    "# Read the original JSON file\n",
    "with open(input_file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Dictionary to store combined paths by SHA\n",
    "combined_data = defaultdict(lambda: {'paths': [], 'detectors': {}, 'segmentation_masks': {}, 'metadata': {}})\n",
    "\n",
    "# Process each entry in the original JSON\n",
    "for entry in data:\n",
    "    sha = entry['id_sha_256']\n",
    "    # Extend the paths list for each SHA identifier\n",
    "    combined_data[sha]['paths'].extend(entry['paths'])\n",
    "    # Merge detector and segmentation mask data\n",
    "    combined_data[sha]['detectors'] = entry['detectors']\n",
    "    combined_data[sha]['segmentation_masks'] = entry['segmentation_masks']\n",
    "    # Combine metadata\n",
    "    combined_data[sha]['metadata'] = entry['metadata']\n",
    "\n",
    "# Convert the dictionary to a list of dictionaries for saving in JSON\n",
    "result = []\n",
    "for sha, details in combined_data.items():\n",
    "    result.append({\n",
    "        'id_sha_256': sha,\n",
    "        'paths': details['paths'],\n",
    "        'detectors': details['detectors'],\n",
    "        'segmentation_masks': details['segmentation_masks'],\n",
    "        'metadata': details['metadata']\n",
    "    })\n",
    "\n",
    "# Save the result to a new JSON file\n",
    "with open(output_file_path, 'w') as file:\n",
    "    json.dump(result, file, indent=4)\n",
    "\n",
    "print(f'Combined file saved to {output_file_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Review of json<b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with open('Dragonpathsmerged.json', 'r') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_sha_256</th>\n",
       "      <th>paths</th>\n",
       "      <th>detectors</th>\n",
       "      <th>segmentation_masks</th>\n",
       "      <th>metadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38cfdcc6f01fb3a5543d1230c12161e554ce3c42b328d0...</td>\n",
       "      <td>[original_db/Pictures 2004.2008/Processed Imag...</td>\n",
       "      <td>{'megadetectorV5': {'researcher': 'Oscar Cajam...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'image': {'Image Make': 'G6.1     ', 'Image M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7ed0d9e3a078a3cd5602b234af3dd79a7dca60f6d20a02...</td>\n",
       "      <td>[original_db/Pictures 2004.2008/Processed Imag...</td>\n",
       "      <td>{'megadetectorV5': {'researcher': 'Oscar Cajam...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'image': {'Image Make': 'G6.1     ', 'Image M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2693a8ac41561c6688dcfb6efd39c147c0e4851073ab51...</td>\n",
       "      <td>[original_db/Pictures 2004.2008/Processed Imag...</td>\n",
       "      <td>{'megadetectorV5': {'researcher': 'Oscar Cajam...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'image': {'Image Make': 'G6.1     ', 'Image M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adddc64a964950c26e117e9613d7270b26a5f460861b95...</td>\n",
       "      <td>[original_db/Pictures 2004.2008/Processed Imag...</td>\n",
       "      <td>{'megadetectorV5': {'researcher': 'Oscar Cajam...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'image': {'Image Make': 'G6.1     ', 'Image M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f7f6d0840c927b353051330b268d63434cf318e18d6878...</td>\n",
       "      <td>[original_db/Pictures 2004.2008/Processed Imag...</td>\n",
       "      <td>{'megadetectorV5': {'researcher': 'Oscar Cajam...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'image': {'Image Make': 'G6.1     ', 'Image M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126102</th>\n",
       "      <td>63b259a47c770f62e34c2f54159e669e9942a711a9d22b...</td>\n",
       "      <td>[original_db/Pictures Study On and Off trails ...</td>\n",
       "      <td>{'megadetectorV5': {'researcher': 'Oscar Cajam...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'image': {'Image ImageDescription': '        ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126103</th>\n",
       "      <td>a43a9d2348b24852945c56b2574bdf6702b8ec1af1cd37...</td>\n",
       "      <td>[original_db/Pictures Study On and Off trails ...</td>\n",
       "      <td>{'megadetectorV5': {'researcher': 'Oscar Cajam...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'image': {'Image ImageDescription': '        ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126104</th>\n",
       "      <td>577a258d69b03f2b6962b6899750ae4c3416061f4ceb72...</td>\n",
       "      <td>[original_db/Pictures Study On and Off trails ...</td>\n",
       "      <td>{'megadetectorV5': {'researcher': 'Oscar Cajam...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'image': {'Image ImageDescription': '        ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126105</th>\n",
       "      <td>ccc08732c77ad9eea6802dc3f61c8950d41628eed02890...</td>\n",
       "      <td>[original_db/Pictures Study On and Off trails ...</td>\n",
       "      <td>{'megadetectorV5': {'researcher': 'Oscar Cajam...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'image': {'Image ImageDescription': '        ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126106</th>\n",
       "      <td>cd2d453e4497dd0c064be18db34da52c8e91c7cbe377e5...</td>\n",
       "      <td>[original_db/Pictures Study On and Off trails ...</td>\n",
       "      <td>{'megadetectorV5': {'researcher': 'Oscar Cajam...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'image': {'Image ImageDescription': '        ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>126107 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               id_sha_256  \\\n",
       "0       38cfdcc6f01fb3a5543d1230c12161e554ce3c42b328d0...   \n",
       "1       7ed0d9e3a078a3cd5602b234af3dd79a7dca60f6d20a02...   \n",
       "2       2693a8ac41561c6688dcfb6efd39c147c0e4851073ab51...   \n",
       "3       adddc64a964950c26e117e9613d7270b26a5f460861b95...   \n",
       "4       f7f6d0840c927b353051330b268d63434cf318e18d6878...   \n",
       "...                                                   ...   \n",
       "126102  63b259a47c770f62e34c2f54159e669e9942a711a9d22b...   \n",
       "126103  a43a9d2348b24852945c56b2574bdf6702b8ec1af1cd37...   \n",
       "126104  577a258d69b03f2b6962b6899750ae4c3416061f4ceb72...   \n",
       "126105  ccc08732c77ad9eea6802dc3f61c8950d41628eed02890...   \n",
       "126106  cd2d453e4497dd0c064be18db34da52c8e91c7cbe377e5...   \n",
       "\n",
       "                                                    paths  \\\n",
       "0       [original_db/Pictures 2004.2008/Processed Imag...   \n",
       "1       [original_db/Pictures 2004.2008/Processed Imag...   \n",
       "2       [original_db/Pictures 2004.2008/Processed Imag...   \n",
       "3       [original_db/Pictures 2004.2008/Processed Imag...   \n",
       "4       [original_db/Pictures 2004.2008/Processed Imag...   \n",
       "...                                                   ...   \n",
       "126102  [original_db/Pictures Study On and Off trails ...   \n",
       "126103  [original_db/Pictures Study On and Off trails ...   \n",
       "126104  [original_db/Pictures Study On and Off trails ...   \n",
       "126105  [original_db/Pictures Study On and Off trails ...   \n",
       "126106  [original_db/Pictures Study On and Off trails ...   \n",
       "\n",
       "                                                detectors segmentation_masks  \\\n",
       "0       {'megadetectorV5': {'researcher': 'Oscar Cajam...                 {}   \n",
       "1       {'megadetectorV5': {'researcher': 'Oscar Cajam...                 {}   \n",
       "2       {'megadetectorV5': {'researcher': 'Oscar Cajam...                 {}   \n",
       "3       {'megadetectorV5': {'researcher': 'Oscar Cajam...                 {}   \n",
       "4       {'megadetectorV5': {'researcher': 'Oscar Cajam...                 {}   \n",
       "...                                                   ...                ...   \n",
       "126102  {'megadetectorV5': {'researcher': 'Oscar Cajam...                 {}   \n",
       "126103  {'megadetectorV5': {'researcher': 'Oscar Cajam...                 {}   \n",
       "126104  {'megadetectorV5': {'researcher': 'Oscar Cajam...                 {}   \n",
       "126105  {'megadetectorV5': {'researcher': 'Oscar Cajam...                 {}   \n",
       "126106  {'megadetectorV5': {'researcher': 'Oscar Cajam...                 {}   \n",
       "\n",
       "                                                 metadata  \n",
       "0       {'image': {'Image Make': 'G6.1     ', 'Image M...  \n",
       "1       {'image': {'Image Make': 'G6.1     ', 'Image M...  \n",
       "2       {'image': {'Image Make': 'G6.1     ', 'Image M...  \n",
       "3       {'image': {'Image Make': 'G6.1     ', 'Image M...  \n",
       "4       {'image': {'Image Make': 'G6.1     ', 'Image M...  \n",
       "...                                                   ...  \n",
       "126102  {'image': {'Image ImageDescription': '        ...  \n",
       "126103  {'image': {'Image ImageDescription': '        ...  \n",
       "126104  {'image': {'Image ImageDescription': '        ...  \n",
       "126105  {'image': {'Image ImageDescription': '        ...  \n",
       "126106  {'image': {'Image ImageDescription': '        ...  \n",
       "\n",
       "[126107 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json('Dragonpathsmerged.json')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126107, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "json structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_structure(obj, indent=0, prefix=\"\"):\n",
    "    # Unicode characters for tree structure\n",
    "    line = \"│   \"\n",
    "    branch = \"├── \"\n",
    "    last_branch = \"└── \"\n",
    "\n",
    "    # Determine branch type (continuing or last)\n",
    "    if indent == 0:\n",
    "        next_prefix = \"\"\n",
    "    else:\n",
    "        next_prefix = prefix + line if prefix.endswith(line) else prefix\n",
    "\n",
    "    # If the object is a DataFrame\n",
    "    if isinstance(obj, pd.DataFrame):\n",
    "        print(f\"{prefix}DataFrame {obj.shape}\")\n",
    "        last_index = len(obj.columns) - 1\n",
    "        for i, column in enumerate(obj.columns):\n",
    "            new_prefix = prefix + (last_branch if i == last_index else branch)\n",
    "            print(f\"{new_prefix}Column '{column}':\")\n",
    "            pretty_print_structure(obj[column], indent + 1, next_prefix + (line if i != last_index else \"    \"))\n",
    "\n",
    "    # If the object is a Series\n",
    "    elif isinstance(obj, pd.Series):\n",
    "        print(f\"{prefix}Series {obj.name} - {obj.dtype}\")\n",
    "        unique_types = obj.apply(type).unique()  # Check types in this column\n",
    "        if len(unique_types) == 1 and unique_types[0] in [int, float, str]:\n",
    "            print(f\"{prefix + branch}All elements of type: {unique_types[0]}\")\n",
    "        else:\n",
    "            last_index = len(obj) - 1\n",
    "            for i, val in enumerate(obj):\n",
    "                new_prefix = prefix + (last_branch if i == last_index else branch)\n",
    "                pretty_print_structure(val, indent + 1, next_prefix + (line if i != last_index else \"    \"))\n",
    "\n",
    "    # If the object is a list\n",
    "    elif isinstance(obj, list):\n",
    "        print(f\"{prefix}List: Length {len(obj)}\")\n",
    "        last_index = len(obj) - 1\n",
    "        for i, item in enumerate(obj):\n",
    "            new_prefix = prefix + (last_branch if i == last_index else branch)\n",
    "            pretty_print_structure(item, indent + 1, next_prefix + (line if i != last_index else \"    \"))\n",
    "\n",
    "    # If the object is a dictionary\n",
    "    elif isinstance(obj, dict):\n",
    "        print(f\"{prefix}Dictionary with {len(obj)} keys\")\n",
    "        last_index = len(list(obj.keys())) - 1\n",
    "        for i, (key, value) in enumerate(obj.items()):\n",
    "            new_prefix = prefix + (last_branch if i == last_index else branch)\n",
    "            print(f\"{new_prefix}Key '{key}':\")\n",
    "            pretty_print_structure(value, indent + 1, next_prefix + (line if i != last_index else \"    \"))\n",
    "\n",
    "    # Other data types (e.g., int, float, str)\n",
    "    else:\n",
    "        print(f\"{prefix}{type(obj).__name__}: {obj}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series 0 - object\n",
      "│   str: 38cfdcc6f01fb3a5543d1230c12161e554ce3c42b328d036b36536ed883e3ca6\n",
      "│   List: Length 2\n",
      "│   │   │   str: original_db/Pictures 2004.2008/Processed Images/Ocelot Paper Data/Ocelots 2010-2011/013L-2P/P150-A-10-06-04-0145-Leopar.JPG\n",
      "│   │       str: original_db/Pictures 2009-2011/P 150/P150-A/P150-A-10-07-10/P150-A-10-06-04-0145-Leopar.JPG\n",
      "│   Dictionary with 3 keys\n",
      "│   ├── Key 'megadetectorV5':\n",
      "│   │   │   Dictionary with 2 keys\n",
      "│   │   │   ├── Key 'researcher':\n",
      "│   │   │   │   │   str: Oscar Cajamarca, Jenner Baquero\n",
      "│   │   │   └── Key 'output':\n",
      "│   │   │   │       Dictionary with 2 keys\n",
      "│   │   │   │       ├── Key 'file':\n",
      "│   │   │   │       │   str: /media/databases/tiputini/original_db/Pictures 2009-2011/P 150/P150-A/P150-A-10-07-10/P150-A-10-06-04-0145-Leopar.JPG\n",
      "│   │   │   │       └── Key 'detections':\n",
      "│   │   │   │           List: Length 1\n",
      "│   │   │   │               Dictionary with 5 keys\n",
      "│   │   │   │               ├── Key 'category':\n",
      "│   │   │   │               │   List: Length 1\n",
      "│   │   │   │               │   │       int: 0\n",
      "│   │   │   │               ├── Key 'confidence':\n",
      "│   │   │   │               │   List: Length 1\n",
      "│   │   │   │               │   │       float: 0.9659069180488581\n",
      "│   │   │   │               ├── Key 'bbox':\n",
      "│   │   │   │               │   List: Length 1\n",
      "│   │   │   │               │   │       List: Length 4\n",
      "│   │   │   │               │   │       │   int: 757\n",
      "│   │   │   │               │   │       │   int: 485\n",
      "│   │   │   │               │   │       │   int: 1955\n",
      "│   │   │   │               │   │           int: 1021\n",
      "│   │   │   │               ├── Key 'detection_categories':\n",
      "│   │   │   │               │   Dictionary with 3 keys\n",
      "│   │   │   │               │   ├── Key '0':\n",
      "│   │   │   │               │   │   │   str: animal\n",
      "│   │   │   │               │   ├── Key '1':\n",
      "│   │   │   │               │   │   │   str: person\n",
      "│   │   │   │               │   └── Key '2':\n",
      "│   │   │   │               │   │       str: vehicle\n",
      "│   │   │   │               └── Key 'info':\n",
      "│   │   │   │                   Dictionary with 4 keys\n",
      "│   │   │   │                   ├── Key 'detection_completion_time':\n",
      "│   │   │   │                   │   str: 2024-05-08 20:00:00\n",
      "│   │   │   │                   ├── Key 'format_version':\n",
      "│   │   │   │                   │   str: 1.3\n",
      "│   │   │   │                   ├── Key 'detector':\n",
      "│   │   │   │                   │   str: md_v5a.0.0.pt\n",
      "│   │   │   │                   └── Key 'detector_metadata':\n",
      "│   │   │   │                       Dictionary with 3 keys\n",
      "│   │   │   │                       ├── Key 'megadetector_version':\n",
      "│   │   │   │                       │   str: v5a.0.0\n",
      "│   │   │   │                       ├── Key 'typical_detection_threshold':\n",
      "│   │   │   │                       │   float: 0.2\n",
      "│   │   │   │                       └── Key 'conservative_detection_threshold':\n",
      "│   │   │   │                           float: 0.05\n",
      "│   ├── Key 'another_detector':\n",
      "│   │   │   Dictionary with 2 keys\n",
      "│   │   │   ├── Key 'researcher':\n",
      "│   │   │   │   │   Dictionary with 0 keys\n",
      "│   │   │   └── Key 'output':\n",
      "│   │   │   │       Dictionary with 0 keys\n",
      "│   └── Key 'ground_truth':\n",
      "│   │       Dictionary with 2 keys\n",
      "│   │       ├── Key 'researcher':\n",
      "│   │       │   Dictionary with 0 keys\n",
      "│   │       └── Key 'output':\n",
      "│   │           Dictionary with 0 keys\n",
      "│   Dictionary with 0 keys\n",
      "    Dictionary with 2 keys\n",
      "    ├── Key 'image':\n",
      "    │   Dictionary with 39 keys\n",
      "    │   ├── Key 'Image Make':\n",
      "    │   │   │   str: G6.1     \n",
      "    │   ├── Key 'Image Model':\n",
      "    │   │   │   str: CUDDEBACK\n",
      "    │   ├── Key 'Image Orientation':\n",
      "    │   │   │   str: Horizontal (normal)\n",
      "    │   ├── Key 'Image XResolution':\n",
      "    │   │   │   str: 72\n",
      "    │   ├── Key 'Image YResolution':\n",
      "    │   │   │   str: 72\n",
      "    │   ├── Key 'Image ResolutionUnit':\n",
      "    │   │   │   str: Pixels/Inch\n",
      "    │   ├── Key 'Image Software':\n",
      "    │   │   │   str: Fw ver A11 B11 P02\n",
      "    │   ├── Key 'Image DateTime':\n",
      "    │   │   │   str: 2010:06:04 01:45:00\n",
      "    │   ├── Key 'Image YCbCrPositioning':\n",
      "    │   │   │   str: Co-sited\n",
      "    │   ├── Key 'Image Copyright':\n",
      "    │   │   │   str: Copyright Cuddeback,2008\n",
      "    │   ├── Key 'Image XPAuthor':\n",
      "    │   │   │   str: NonTypical, Inc.\n",
      "    │   ├── Key 'Image XPTitle':\n",
      "    │   │   │   str: Capture  \n",
      "    │   ├── Key 'Image XPComment':\n",
      "    │   │   │   str: A11 B11 P02 | 2010:06:04  01:45:00 | Exposure:  170 mS    5 |   0 D| 00E51F43 | 05DE8F |\n",
      "    │   ├── Key 'Image ExifOffset':\n",
      "    │   │   │   str: 486\n",
      "    │   ├── Key 'EXIF ExposureTime':\n",
      "    │   │   │   str: 17/100\n",
      "    │   ├── Key 'EXIF FNumber':\n",
      "    │   │   │   str: 16/5\n",
      "    │   ├── Key 'EXIF ExposureProgram':\n",
      "    │   │   │   str: Program Normal\n",
      "    │   ├── Key 'EXIF ISOSpeedRatings':\n",
      "    │   │   │   str: 100\n",
      "    │   ├── Key 'EXIF ExifVersion':\n",
      "    │   │   │   str: 0210\n",
      "    │   ├── Key 'EXIF DateTimeOriginal':\n",
      "    │   │   │   str: 2010:06:04 01:45:00\n",
      "    │   ├── Key 'EXIF DateTimeDigitized':\n",
      "    │   │   │   str: 2010:06:04 01:45:00\n",
      "    │   ├── Key 'EXIF ComponentsConfiguration':\n",
      "    │   │   │   str: YCbCr\n",
      "    │   ├── Key 'EXIF ShutterSpeedValue':\n",
      "    │   │   │   str: 17/100\n",
      "    │   ├── Key 'EXIF ApertureValue':\n",
      "    │   │   │   str: 16/5\n",
      "    │   ├── Key 'EXIF ExposureBiasValue':\n",
      "    │   │   │   str: 0\n",
      "    │   ├── Key 'EXIF MaxApertureValue':\n",
      "    │   │   │   str: 19/50\n",
      "    │   ├── Key 'EXIF MeteringMode':\n",
      "    │   │   │   str: Average\n",
      "    │   ├── Key 'EXIF Flash':\n",
      "    │   │   │   str: Flash did not fire\n",
      "    │   ├── Key 'EXIF FocalLength':\n",
      "    │   │   │   str: 19/2\n",
      "    │   ├── Key 'EXIF ColorSpace':\n",
      "    │   │   │   str: sRGB\n",
      "    │   ├── Key 'EXIF ExifImageWidth':\n",
      "    │   │   │   str: 2048\n",
      "    │   ├── Key 'EXIF ExifImageLength':\n",
      "    │   │   │   str: 1536\n",
      "    │   ├── Key 'EXIF RelatedSoundFile':\n",
      "    │   │   │   str: \n",
      "    │   ├── Key 'Interoperability InteroperabilityIndex':\n",
      "    │   │   │   str: R98\n",
      "    │   ├── Key 'Interoperability InteroperabilityVersion':\n",
      "    │   │   │   str: 0100\n",
      "    │   ├── Key 'EXIF InteroperabilityOffset':\n",
      "    │   │   │   str: 3842\n",
      "    │   ├── Key 'EXIF SensingMethod':\n",
      "    │   │   │   str: One-chip color area\n",
      "    │   ├── Key 'EXIF FileSource':\n",
      "    │   │   │   str: Digital Camera\n",
      "    │   └── Key 'EXIF SceneType':\n",
      "    │   │       str: Directly Photographed\n",
      "    └── Key 'other':\n",
      "        Dictionary with 2 keys\n",
      "        ├── Key 'location':\n",
      "        │   Dictionary with 0 keys\n",
      "        └── Key 'label_location':\n",
      "            Dictionary with 0 keys\n"
     ]
    }
   ],
   "source": [
    "pretty_print_structure(df.loc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Modifications**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Diccionario con coordenadas para diferentes patrones de paths\n",
    "coordinates_dict = {\n",
    "    \"H750\": {\"latitude\": \"0.63242S\", \"longitude\": \"76.14507W\"},\n",
    "    \"H1650\": {\"latitude\": \"0.62563S\", \"longitude\": \"76.14572W\"},\n",
    "    \"H3000\": {\"latitude\": \"0.62616S\", \"longitude\": \"76.15614W\"},\n",
    "    \"M1400\": {\"latitude\": \"0.6194S\", \"longitude\": \"76.16307W\"},\n",
    "    \"M2200\": {\"latitude\": \"0.62245S\", \"longitude\": \"76.16823W\"},\n",
    "    \"M3350\": {\"latitude\": \"0.62782S\", \"longitude\": \"76.16973W\"},\n",
    "    \"M4250\": {\"latitude\": \"0.63426S\", \"longitude\": \"76.16458W\"},\n",
    "    \"P150\": {\"latitude\": \"0.64107S\", \"longitude\": \"76.14355W\"},\n",
    "    \"P1150\": {\"latitude\": \"0.64533S\", \"longitude\": \"76.13853W\"},\n",
    "    \"P2450\": {\"latitude\": \"0.64986S\", \"longitude\": \"76.14212W\"},\n",
    "    \"Pu275\": {\"latitude\": \"0.63233S\", \"longitude\": \"76.1558W\"},\n",
    "    \"Pu 275\": {\"latitude\": \"0.63233S\", \"longitude\": \"76.1558W\"},\n",
    "    \"H2-200\": {\"latitude\": \"0.6333S\", \"longitude\": \"76.14209W\"},\n",
    "    \"H2-400\": {\"latitude\": \"0.63316S\", \"longitude\": \"76.14022W\"},\n",
    "    \"H2-600\": {\"latitude\": \"0.63312S\", \"longitude\": \"76.1384W\"},\n",
    "    \"H2-800\": {\"latitude\": \"0.6331S\", \"longitude\": \"76.13669W\"},\n",
    "    \"H4-200\": {\"latitude\": \"0.63151S\", \"longitude\": \"76.14203W\"},\n",
    "    \"H4-400\": {\"latitude\": \"0.63143S\", \"longitude\": \"76.14027W\"},\n",
    "    \"H4-600\": {\"latitude\": \"0.63136S\", \"longitude\": \"76.13839W\"},\n",
    "    \"H4-800\": {\"latitude\": \"0.63131S\", \"longitude\": \"76.13675W\"},\n",
    "    \"H6-200\": {\"latitude\": \"0.6297S\", \"longitude\": \"76.14197W\"},\n",
    "    \"H6-400\": {\"latitude\": \"0.62965S\", \"longitude\": \"76.14018W\"},\n",
    "    \"H6-600\": {\"latitude\": \"0.62954S\", \"longitude\": \"76.13842W\"},\n",
    "    \"H6-800\": {\"latitude\": \"0.62944S\", \"longitude\": \"76.13665W\"},\n",
    "    \"H8-200\": {\"latitude\": \"0.62778S\", \"longitude\": \"76.14207W\"},\n",
    "    \"H8-400\": {\"latitude\": \"0.62783S\", \"longitude\": \"76.1404W\"},\n",
    "    \"H8-600\": {\"latitude\": \"0.62775S\", \"longitude\": \"76.13861W\"},\n",
    "    \"H8-800\": {\"latitude\": \"0.62774S\", \"longitude\": \"76.13681W\"},\n",
    "    \"P-1-200\": {\"latitude\": \"0.62254S\", \"longitude\": \"76.15947W\"},\n",
    "    \"P-1-400\": {\"latitude\": \"0.62262S\", \"longitude\": \"76.16031W\"},\n",
    "    \"P-1-600\": {\"latitude\": \"0.6228S\", \"longitude\": \"76.16267W\"},\n",
    "    \"P-1-800\": {\"latitude\": \"0.62284S\", \"longitude\": \"76.16467W\"},\n",
    "    \"P1-200\": {\"latitude\": \"0.62455S\", \"longitude\": \"76.15943W\"},\n",
    "    \"P1-400\": {\"latitude\": \"0.6246S\", \"longitude\": \"76.16118W\"},\n",
    "    \"P1-600\": {\"latitude\": \"0.62476S\", \"longitude\": \"76.16291W\"},\n",
    "    \"P1-800\": {\"latitude\": \"0.62469S\", \"longitude\": \"76.16443W\"},\n",
    "    \"P3-200\": {\"latitude\": \"0.62463S\", \"longitude\": \"76.15939W\"},\n",
    "    \"P3-400\": {\"latitude\": \"0.62632S\", \"longitude\": \"76.16024W\"},\n",
    "    \"P3-600\": {\"latitude\": \"0.62654S\", \"longitude\": \"76.16265W\"},\n",
    "    \"P3-800\": {\"latitude\": \"0.62664S\", \"longitude\": \"76.16454W\"},\n",
    "    \"P5-200\": {\"latitude\": \"0.62812S\", \"longitude\": \"76.15895W\"},\n",
    "    \"P5-400\": {\"latitude\": \"0.62829S\", \"longitude\": \"76.16074W\"},\n",
    "    \"P5-600\": {\"latitude\": \"0.62816S\", \"longitude\": \"76.16268W\"},\n",
    "    \"P5-800\": {\"latitude\": \"0.62838S\", \"longitude\": \"76.16445W\"},\n",
    "    \"Chorongo\": {\"latitude\": \"0.63453S\", \"longitude\": \"76.148W\"}\n",
    "}\n",
    "\n",
    "# Diccionario con información para notes_location\n",
    "notes_location_dict = {\n",
    "    \"H750\": [\"H750\", \"Trail cameras\"],\n",
    "    \"H1650\": [\"H1650\", \"Trail cameras\"],\n",
    "    \"H3000\": [\"H3000\", \"Trail cameras\"],\n",
    "    \"M1400\": [\"M1400\", \"Trail cameras\"],\n",
    "    \"M2200\": [\"M2200\", \"Trail cameras\"],\n",
    "    \"M3350\": [\"M3350\", \"Trail cameras\"],\n",
    "    \"M4250\": [\"M4250\", \"Trail cameras\"],\n",
    "    \"P150\": [\"P150\", \"Trail cameras\"],\n",
    "    \"P1150\": [\"P1150\", \"Trail cameras\"],\n",
    "    \"P2450\": [\"P2450\", \"Trail cameras\"],\n",
    "    \"Pu275\": [\"Pu275\", \"Trail cameras\"],\n",
    "    \"Pu 275\": [\"Pu 275\", \"Trail cameras\"],\n",
    "    \"H2-200\": [\"H2-200\", \"Plot cameras\"],\n",
    "    \"H2-400\": [\"H2-400\", \"Plot cameras\"],\n",
    "    \"H2-600\": [\"H2-600\", \"Plot cameras\"],\n",
    "    \"H2-800\": [\"H2-800\", \"Plot cameras\"],\n",
    "    \"H4-200\": [\"H4-200\", \"Plot cameras\"],\n",
    "    \"H4-400\": [\"H4-400\", \"Plot cameras\"],\n",
    "    \"H4-600\": [\"H4-600\", \"Plot cameras\"],\n",
    "    \"H4-800\": [\"H4-800\", \"Plot cameras\"],\n",
    "    \"H6-200\": [\"H6-200\", \"Plot cameras\"],\n",
    "    \"H6-400\": [\"H6-400\", \"Plot cameras\"],\n",
    "    \"H6-600\": [\"H6-600\", \"Plot cameras\"],\n",
    "    \"H6-800\": [\"H6-800\", \"Plot cameras\"],\n",
    "    \"H8-200\": [\"H8-200\", \"Plot cameras\"],\n",
    "    \"H8-400\": [\"H8-400\", \"Plot cameras\"],\n",
    "    \"H8-600\": [\"H8-600\", \"Plot cameras\"],\n",
    "    \"H8-800\": [\"H8-800\", \"Plot cameras\"],\n",
    "    \"P-1-200\": [\"P-1-200\", \"Plot cameras\"],\n",
    "    \"P-1-400\": [\"P-1-400\", \"Plot cameras\"],\n",
    "    \"P-1-600\": [\"P-1-600\", \"Plot cameras\"],\n",
    "    \"P-1-800\": [\"P-1-800\", \"Plot cameras\"],\n",
    "    \"P1-200\": [\"P1-200\", \"Plot cameras\"],\n",
    "    \"P1-400\": [\"P1-400\", \"Plot cameras\"],\n",
    "    \"P1-600\": [\"P1-600\", \"Plot cameras\"],\n",
    "    \"P1-800\": [\"P1-800\", \"Plot cameras\"],\n",
    "    \"P3-200\": [\"P3-200\", \"Plot cameras\"],\n",
    "    \"P3-400\": [\"P3-400\", \"Plot cameras\"],\n",
    "    \"P3-600\": [\"P3-600\", \"Plot cameras\"],\n",
    "    \"P3-800\": [\"P3-800\", \"Plot cameras\"],\n",
    "    \"P5-200\": [\"P5-200\", \"Plot cameras\"],\n",
    "    \"P5-400\": [\"P5-400\", \"Plot cameras\"],\n",
    "    \"P5-600\": [\"P5-600\", \"Plot cameras\"],\n",
    "    \"P5-800\": [\"P5-800\", \"Plot cameras\"],\n",
    "    \"Chorongo\": [\"Chorongo\", \"Saladeros\"]\n",
    "}\n",
    "\n",
    "# Función para extraer coordenadas del path\n",
    "def get_coordinates(path):\n",
    "    for key in coordinates_dict:\n",
    "        if key in path:\n",
    "            return coordinates_dict[key]\n",
    "    return {}\n",
    "\n",
    "# Función para obtener notes_location del path\n",
    "def get_notes_location(path):\n",
    "    for key in notes_location_dict:\n",
    "        if key in path:\n",
    "            return notes_location_dict[key]\n",
    "    return []\n",
    "\n",
    "# Cargar el archivo JSON\n",
    "with open('/home/ocajamarca/python/anaconda3/project/Megadetector/megadetector_metadata.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Modificar cada entrada en el JSON\n",
    "for entry in data:\n",
    "    # Cambiar label_location por notes_location y actualizar el valor\n",
    "    if 'label_location' in entry['metadata']['other']:\n",
    "        entry['metadata']['other'].pop('label_location')\n",
    "        entry['metadata']['other']['notes_location (salar, bosque, rio)'] = \"\"\n",
    "    \n",
    "    # Añadir coordenadas a location y notes_location si algún path contiene un patrón conocido\n",
    "    for path in entry['paths']:\n",
    "        coordinates = get_coordinates(path)\n",
    "        notes_location = get_notes_location(path)\n",
    "        if coordinates:\n",
    "            entry['metadata']['other']['location'] = coordinates\n",
    "        if notes_location:\n",
    "            entry['metadata']['other']['notes_location (salar, bosque, rio)'] = f\"{notes_location}\"\n",
    "            break\n",
    "\n",
    "# Guardar el archivo JSON modificado\n",
    "with open('/home/ocajamarca/python/anaconda3/project/Tests/data_modified_2.json', 'w') as file:\n",
    "    json.dump(data, file, indent=4)\n",
    "\n",
    "print(\"Archivo JSON modificado y guardado como 'data_modified_2.json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('/home/ocajamarca/python/anaconda3/project/Megadetector/Dragonpathsmerged.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "    \n",
    "names = set()\n",
    "\n",
    "for item in data:\n",
    "    for path in item.get('paths', []):\n",
    "        if 'H2-200' in path:\n",
    "            start_idx = path.find(\"H2-200\")\n",
    "            end_idx = path.find(\"/\", start_idx)\n",
    "            if end_idx == -1:\n",
    "                end_idx = len(path)\n",
    "            animal_name = path[start_idx:end_idx]\n",
    "            names.add(animal_name)\n",
    "\n",
    "names = sorted(names)\n",
    "\n",
    "for i, name in enumerate(names, 1):\n",
    "    print(f\"{i}. {name}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
